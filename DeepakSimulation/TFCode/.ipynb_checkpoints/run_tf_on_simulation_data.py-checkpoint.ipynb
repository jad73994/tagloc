{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import scipy.io as spio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Import Data from Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "744\n"
     ]
    }
   ],
   "source": [
    "mat = spio.loadmat('../ReflectionModel/dataset_2.mat')\n",
    "features = mat['features']\n",
    "labels = mat['labels']\n",
    "print(len(features))\n",
    "print(len(features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data to make it 0-mean, 1 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(744,)\n",
      "(100000, 744)\n",
      "(100000, 744)\n",
      "(100000, 2)\n",
      "[ 5.75757782  4.33572888]\n",
      "[ 1.50315946  1.50109093  1.4988298   1.49266982  1.48432403  1.47321853\n",
      "  1.45962753  1.44957145  1.43547674  1.42550123  1.41776869  1.40891897\n",
      "  1.40464574  1.40081011  1.39350686  1.3876605   1.38834796  1.38462617\n",
      "  1.38282034  1.37929115  1.37467546  1.36807602  1.36257241  1.36075581\n",
      "  1.35685574  1.35138349  1.34216772  1.33444303  1.3243377   1.31200297\n",
      "  1.29890701  1.48734615  1.48479231  1.48126276  1.47927189  1.47873322\n",
      "  1.47363626  1.47069967  1.47064985  1.47103399  1.46945672  1.46635492\n",
      "  1.46368271  1.45621113  1.44749263  1.44078627  1.43670867  1.43340685\n",
      "  1.4299623   1.4269948   1.42076747  1.41534622  1.40888781  1.40179645\n",
      "  1.39467555  1.38957635  1.38892293  1.38715744  1.38159447  1.37412147\n",
      "  1.36516209  1.35865435  1.86039562  1.84183167  1.8274101   1.81269607\n",
      "  1.79934722  1.78957177  1.78205275  1.77188538  1.7611249   1.74892173\n",
      "  1.73490111  1.72492656  1.7168491   1.70649949  1.69336992  1.68001034\n",
      "  1.66951048  1.65945707  1.64609551  1.63373737  1.62212856  1.61164676\n",
      "  1.6002171   1.59313509  1.58363206  1.57522532  1.56687299  1.55996765\n",
      "  1.5518334   1.5433333   1.53394917  1.67620341  1.6706      1.66722331\n",
      "  1.66691381  1.66413063  1.65846909  1.65250379  1.64611847  1.64040373\n",
      "  1.63639823  1.63460168  1.62921741  1.62706554  1.62103379  1.61421673\n",
      "  1.60812408  1.60094143  1.5943891   1.58644085  1.5761027   1.56613345\n",
      "  1.55512864  1.54913204  1.53989759  1.5330214   1.52766477  1.51777628\n",
      "  1.51015413  1.50441535  1.50069783  1.49639636  1.64049225  1.63360127\n",
      "  1.6272581   1.6204577   1.61349309  1.61012833  1.60589933  1.59912088\n",
      "  1.59255954  1.58448362  1.57997066  1.5755321   1.57121733  1.56710699\n",
      "  1.56243945  1.55774213  1.55386512  1.54714875  1.54347302  1.54152111\n",
      "  1.54049593  1.53589026  1.53196872  1.52877287  1.52575643  1.52357798\n",
      "  1.51876377  1.5152143   1.51425103  1.51169642  1.50610375  1.64159451\n",
      "  1.63683952  1.63151935  1.6229891   1.61620825  1.60462789  1.5938856\n",
      "  1.58282721  1.57210465  1.56152943  1.55258617  1.54417418  1.53395898\n",
      "  1.5233088   1.51264498  1.50703244  1.50656133  1.5043361   1.50197531\n",
      "  1.49731014  1.49470717  1.49174775  1.48581706  1.47802547  1.4732194\n",
      "  1.46515427  1.45933922  1.45509521  1.44729801  1.44014233  1.43286831\n",
      "  1.65565356  1.6519178   1.65299135  1.64967213  1.65058984  1.64898455\n",
      "  1.6458768   1.64277633  1.63659521  1.63176281  1.62711595  1.62377311\n",
      "  1.61619712  1.61049036  1.60358799  1.59667288  1.59107295  1.59195848\n",
      "  1.59136702  1.59134341  1.59253236  1.59271428  1.59168339  1.59100269\n",
      "  1.5888244   1.58331165  1.58111531  1.57811541  1.57059827  1.56264609\n",
      "  1.55611102  1.96174272  1.94855758  1.93719007  1.92635737  1.91864507\n",
      "  1.90970988  1.89719118  1.88478077  1.87569444  1.86619989  1.85828538\n",
      "  1.84920823  1.837978    1.82694656  1.81613684  1.80784708  1.79894598\n",
      "  1.78715818  1.77607177  1.76408672  1.75167233  1.74291146  1.73883963\n",
      "  1.73282702  1.72569489  1.72027972  1.71459358  1.70909452  1.70105224\n",
      "  1.69234652  1.68375257  1.66032219  1.65143736  1.64381643  1.63554498\n",
      "  1.62788331  1.61955307  1.61402173  1.61030915  1.60413822  1.59841851\n",
      "  1.59581012  1.58977324  1.58338685  1.57714342  1.57228558  1.56771805\n",
      "  1.56351613  1.55759921  1.55579888  1.55234227  1.54823816  1.5407871\n",
      "  1.53106008  1.52373454  1.51747659  1.51107275  1.5034967   1.49796581\n",
      "  1.49656024  1.49109799  1.48926531  1.63774458  1.63888256  1.63896807\n",
      "  1.63469714  1.62786353  1.62352538  1.61875403  1.61538615  1.60930817\n",
      "  1.60174845  1.59611346  1.58960424  1.58447084  1.57802759  1.57625318\n",
      "  1.57674261  1.57846632  1.5718483   1.56585731  1.55897064  1.55628703\n",
      "  1.55428864  1.55081539  1.54989736  1.54901438  1.54695833  1.54417359\n",
      "  1.5412055   1.53766884  1.53237687  1.52849102  1.74493009  1.73841124\n",
      "  1.73198494  1.7274207   1.72547629  1.72186811  1.72048451  1.716304\n",
      "  1.70983652  1.70601775  1.70032347  1.69457586  1.68711213  1.67864346\n",
      "  1.66849675  1.66190267  1.65749007  1.65370008  1.65229645  1.64932084\n",
      "  1.6427356   1.63807014  1.63348981  1.62568842  1.61744766  1.60843796\n",
      "  1.60153653  1.5912863   1.58135396  1.57378982  1.56591255  2.03192217\n",
      "  2.01920956  2.0064821   1.99166897  1.97410439  1.95968198  1.94732955\n",
      "  1.93422227  1.92262359  1.91353376  1.90394984  1.89270355  1.8818348\n",
      "  1.87173558  1.86175211  1.85177458  1.8403812   1.83446383  1.82678974\n",
      "  1.81919013  1.81175702  1.80491651  1.7954975   1.78434677  1.77315322\n",
      "  1.76281021  1.75191549  1.7432965   1.73388092  1.7247004   1.71573872\n",
      "  2.10525425  2.09340579  2.08191185  2.07329069  2.06657503  2.06161416\n",
      "  2.05865681  2.05330916  2.05069052  2.04531762  2.03871667  2.03280234\n",
      "  2.02382376  2.01454802  2.00798513  2.00028753  1.98830539  1.97933231\n",
      "  1.96922534  1.96030032  1.95220854  1.94580058  1.93846455  1.92897714\n",
      "  1.92065454  1.91345792  1.9093114   1.90412973  1.90051989  1.89872067\n",
      "  1.89721827  1.42557464  1.41592317  1.40717303  1.39707438  1.38560171\n",
      "  1.37884252  1.37011024  1.35832189  1.34561035  1.33565438  1.32724361\n",
      "  1.3183053   1.31498539  1.31309103  1.30908189  1.30230803  1.29457688\n",
      "  1.28715815  1.27921864  1.27531436  1.27005915  1.26644948  1.26379731\n",
      "  1.26084673  1.25594636  1.24603616  1.23744746  1.23311272  1.23110475\n",
      "  1.2310165   1.22803384  2.27654373  2.27571186  2.27209361  2.26818285\n",
      "  2.26364271  2.25645884  2.24742016  2.24069592  2.23444952  2.22946432\n",
      "  2.22626548  2.21964038  2.21174271  2.20573955  2.20231866  2.19847361\n",
      "  2.19310194  2.18706329  2.18409105  2.17991886  2.1757182   2.17045856\n",
      "  2.16606115  2.15872145  2.1530271   2.14662081  2.14034731  2.1330853\n",
      "  2.12663537  2.12084314  2.11576031  1.75546817  1.74700564  1.73627148\n",
      "  1.72288559  1.71194312  1.70373624  1.69609023  1.68899913  1.68133458\n",
      "  1.6718638   1.66054458  1.65289254  1.64213974  1.63503274  1.62928929\n",
      "  1.62254353  1.61726014  1.61125055  1.60658868  1.60453198  1.60215377\n",
      "  1.60114327  1.59497331  1.59206152  1.58712168  1.58083684  1.57884083\n",
      "  1.57479398  1.5688141   1.56127637  1.55434886  1.59886904  1.59259276\n",
      "  1.58554127  1.57902777  1.57293019  1.56324383  1.55451798  1.54830212\n",
      "  1.54230497  1.53793597  1.52986679  1.52162566  1.51386718  1.50570844\n",
      "  1.4981431   1.49068644  1.48260868  1.47775357  1.46932106  1.4594871\n",
      "  1.44864673  1.44190745  1.43452474  1.42597715  1.4175295   1.40819984\n",
      "  1.40184197  1.39452806  1.38426088  1.37562905  1.37041344  1.49595937\n",
      "  1.48742068  1.47966783  1.47561043  1.46975411  1.46923058  1.46805998\n",
      "  1.46688575  1.46577909  1.46434465  1.46139376  1.45766515  1.45635411\n",
      "  1.45540515  1.45462164  1.44845208  1.43730884  1.42778252  1.41879886\n",
      "  1.41209629  1.40339371  1.39497826  1.38982401  1.38673752  1.38084933\n",
      "  1.37840801  1.37336441  1.36685782  1.36437055  1.36111482  1.35821127\n",
      "  1.86971209  1.85920867  1.84452527  1.83399471  1.81934034  1.80707535\n",
      "  1.79652654  1.78625709  1.77856616  1.76979428  1.76079013  1.75090704\n",
      "  1.7450127   1.73743489  1.73119877  1.72506024  1.71763719  1.70453501\n",
      "  1.69238759  1.68027873  1.6668478   1.6546381   1.64339742  1.63196118\n",
      "  1.62180875  1.61533454  1.60567706  1.5966722   1.59216411  1.58831634\n",
      "  1.58316716  2.27069129  2.2657084   2.25899762  2.25225001  2.2428911\n",
      "  2.23466377  2.22944807  2.22457313  2.21679241  2.20962974  2.20113619\n",
      "  2.19363054  2.18829051  2.18272774  2.1772627   2.16971887  2.1627908\n",
      "  2.15849234  2.15340984  2.14939693  2.14570958  2.13925894  2.12893384\n",
      "  2.12025586  2.11261267  2.10380707  2.09533603  2.08668158  2.08027272\n",
      "  2.07441451  2.06884918  1.59620307  1.5916127   1.585646    1.58104818\n",
      "  1.57553729  1.57076758  1.56326385  1.55396187  1.54717744  1.54044115\n",
      "  1.53055487  1.52395126  1.51773704  1.51180167  1.50458054  1.49713054\n",
      "  1.48908893  1.48310463  1.47311063  1.46454757  1.45688908  1.45286925\n",
      "  1.45152697  1.44755269  1.44271339  1.4380637   1.43457332  1.42889418\n",
      "  1.41946225  1.4139467   1.40485676  1.85855425  1.84381168  1.83020562\n",
      "  1.82053756  1.81289007  1.80352175  1.79473162  1.78430947  1.77678256\n",
      "  1.7706692   1.76303644  1.75594599  1.74796175  1.74129753  1.73043877\n",
      "  1.71738836  1.70338139  1.69729254  1.69049756  1.68491237  1.67529983\n",
      "  1.66530315  1.65644728  1.64560035  1.63480081  1.62485637  1.61607666\n",
      "  1.60712585  1.59909605  1.59270889  1.58514543  1.69025569  1.68253532\n",
      "  1.67498005  1.66517296  1.65299395  1.6427391   1.630149    1.62053339\n",
      "  1.61348189  1.60398015  1.59623251  1.5887426   1.58327074  1.57898031\n",
      "  1.57652436  1.57051825  1.56201179  1.55316077  1.5416191   1.53201834\n",
      "  1.52649482  1.51886891  1.51135586  1.50735807  1.50384767  1.50117417\n",
      "  1.49656846  1.49565911  1.494113    1.49049973  1.48703683  2.32391722\n",
      "  2.31810141  2.31246428  2.30873081  2.30707454  2.30309422  2.29751976\n",
      "  2.29267445  2.28679473  2.27869936  2.27112184  2.26516916  2.2590215\n",
      "  2.25221653  2.24536595  2.23915905  2.2336747   2.22408182  2.21582876\n",
      "  2.20777158  2.19949241  2.19110954  2.184757    2.17986481  2.17538416\n",
      "  2.17013135  2.16539471  2.15897847  2.15321154  2.14742484  2.14181888]\n"
     ]
    }
   ],
   "source": [
    "feature_mean = np.mean(features,axis=0)\n",
    "label_mean = np.mean(labels,axis=0)\n",
    "label_std = np.std(labels,axis=0)\n",
    "feature_std = np.std(features,axis=0)\n",
    "print(feature_std.shape)\n",
    "feature_mean_m = np.tile(feature_mean, [len(features),1])\n",
    "feature_std_m = np.tile(feature_std, [len(features),1])\n",
    "label_mean_m = np.tile(label_mean,[len(labels),1])\n",
    "label_std_m = np.tile(label_std,[len(labels),1])\n",
    "print(feature_std_m.shape)\n",
    "features_normal = np.divide(features - feature_mean_m, feature_std_m)\n",
    "labels_normal = np.divide(labels- label_mean_m, label_std_m)\n",
    "print(features_normal.shape)\n",
    "print(labels_normal.shape)\n",
    "print(label_std)\n",
    "print(feature_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.float64'>\n",
      "<type 'numpy.float32'>\n",
      "(60000, 744)\n",
      "[[-0.58843237  1.04194999  1.55480695  0.34897208 -1.17734814 -1.47347283\n",
      "  -0.15485772  1.35086918  1.43462837 -0.06087759]\n",
      " [ 0.5727374  -0.63181102  0.57865942 -0.48796439  0.29917082 -0.0819752\n",
      "  -0.12569059  0.34876576 -0.49538198  0.58206981]\n",
      " [ 0.38792133 -0.09478956 -0.33191782  0.58606082 -0.70387477  0.53310448\n",
      "  -0.23637578 -0.18864956  0.47202417 -0.68435276]\n",
      " [ 0.4830538  -0.41351202  0.27046362 -0.18620938  0.04199331  0.12849844\n",
      "  -0.26418701  0.37733597 -0.46469939  0.56523752]\n",
      " [ 0.23897913  0.29706541 -0.21316612 -0.31462404  0.16421017  0.36679238\n",
      "  -0.11064763 -0.33104613  0.09344901  0.36327749]\n",
      " [ 0.47181526 -0.51018226  0.46705303 -0.4285498   0.29008842 -0.14044641\n",
      "   0.02227816  0.12073169 -0.25261331  0.38072529]\n",
      " [-0.39212355  0.21411426  0.09052867 -0.35841307  0.44956285 -0.35605821\n",
      "   0.10936376  0.16798821 -0.39403951  0.50060964]\n",
      " [ 0.06375302  0.13484873 -0.30906481  0.44539991 -0.59131849  0.60660297\n",
      "  -0.60562086  0.54088175 -0.43001989  0.24821724]\n",
      " [-1.00214684 -0.46104035  0.99861318  0.41289398 -1.01914525 -0.3642562\n",
      "   1.03837085  0.34895554 -1.05346155 -0.34890807]\n",
      " [ 0.34256652 -0.07658372 -0.25627568  0.5011549  -0.6298157   0.62589526\n",
      "  -0.45365131  0.16558865  0.18203819 -0.47946042]]\n"
     ]
    }
   ],
   "source": [
    "train_pts = 60000\n",
    "valid_pts = 20000\n",
    "test_pts = 20000\n",
    "print(type(features_normal[0][0]))\n",
    "features_normal = features_normal.astype('float32')\n",
    "print(type(features_normal[0][0]))\n",
    "labels_normal = labels_normal.astype('float32')\n",
    "train_dataset = (features_normal[0:train_pts ,:])\n",
    "train_labels = labels_normal[0:train_pts,:]\n",
    "valid_dataset = features_normal[train_pts:valid_pts+train_pts,:]\n",
    "valid_labels = labels_normal[train_pts:valid_pts+train_pts,:]\n",
    "test_dataset = features_normal[train_pts+valid_pts:train_pts+valid_pts+test_pts,:]\n",
    "test_labels = labels_normal[train_pts+valid_pts:train_pts+valid_pts+test_pts,:]\n",
    "print(train_dataset.shape)\n",
    "print(train_dataset[0:10,0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "num_hidden = [1024,512,256]\n",
    "depth = 4\n",
    "graph = tf.Graph()\n",
    "keep_prob = 1\n",
    "decay_rate = 0.5\n",
    "decay_steps = 1500\n",
    "num_features =features.shape[1]\n",
    "num_labels = labels.shape[1]\n",
    "print(num_features)\n",
    "def get_logits_for_training(dataset,w,b):\n",
    "    cur_mat = dataset\n",
    "    for i in range(0,depth-1):\n",
    "        cur_mat = tf.matmul(cur_mat,w[i])+b[i]\n",
    "        cur_mat = tf.nn.dropout(tf.nn.relu(cur_mat),keep_prob)\n",
    "    logits = tf.matmul(cur_mat,w[depth-1])+b[depth-1]\n",
    "    return logits\n",
    "\n",
    "def get_logits_for_prediction(dataset,w,b):\n",
    "    cur_mat = dataset\n",
    "    for i in range(0,depth-1):\n",
    "        cur_mat = tf.matmul(cur_mat,w[i])+b[i]\n",
    "        cur_mat = tf.nn.relu(cur_mat)\n",
    "    logits = tf.matmul(cur_mat,w[depth-1])+b[depth-1]\n",
    "    return logits\n",
    "    \n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, num_features))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  w = []\n",
    "  b = []\n",
    "  w.append(tf.Variable(tf.truncated_normal([num_features, num_hidden[0]],stddev=0.01)))\n",
    "  b.append(tf.zeros([num_hidden[0]]))\n",
    "  for i in range(1,depth-1):\n",
    "    w.append(tf.Variable(tf.truncated_normal([num_hidden[i-1],num_hidden[i]], stddev=0.01)))\n",
    "    b.append(tf.zeros([num_hidden[i]]))\n",
    "  w.append(tf.Variable(tf.truncated_normal([num_hidden[depth-2], num_labels],stddev=0.01)))\n",
    "  b.append(tf.Variable(tf.zeros([num_labels])))\n",
    "  \n",
    "  # Training computation.  \n",
    "  logits = get_logits_for_training(tf_train_dataset,w,b)\n",
    "  #loss = tf.reduce_mean(\n",
    "  #  tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(tf_train_labels, logits)))\n",
    "  #Optimizer\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  logits_predict = get_logits_for_prediction(tf_train_dataset,w,b)\n",
    "  train_prediction = (logits_predict)\n",
    "  valid_prediction = (get_logits_for_prediction(tf_valid_dataset,w,b))\n",
    "  test_prediction = (get_logits_for_prediction(tf_test_dataset,w,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 1: 1.002852\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0012638569\n",
      "Minibatch loss at step 101: 0.993133\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.993\n",
      "Validation accuracy: 0.998\n",
      "Test accuracy: 1.0007444620\n",
      "Minibatch loss at step 201: 0.967736\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.968\n",
      "Validation accuracy: 0.969\n",
      "Test accuracy: 0.9694172144\n",
      "Minibatch loss at step 301: 0.990211\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.990\n",
      "Validation accuracy: 1.001\n",
      "Test accuracy: 0.9955815673\n",
      "Minibatch loss at step 401: 0.966359\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.966\n",
      "Validation accuracy: 0.960\n",
      "Test accuracy: 0.9845948219\n",
      "Minibatch loss at step 501: 0.937805\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.938\n",
      "Validation accuracy: 1.100\n",
      "Test accuracy: 0.9303420782\n",
      "Minibatch loss at step 601: 1.002500\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0014362335\n",
      "Minibatch loss at step 701: 0.998541\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0012737513\n",
      "Minibatch loss at step 801: 1.005345\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.005\n",
      "Validation accuracy: 0.998\n",
      "Test accuracy: 1.0011790991\n",
      "Minibatch loss at step 901: 1.002511\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0013133287\n",
      "Minibatch loss at step 1001: 1.005898\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.006\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0013387203\n",
      "Minibatch loss at step 1101: 0.994686\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.995\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0012559891\n",
      "Minibatch loss at step 1201: 1.009313\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.009\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0013182163\n",
      "Minibatch loss at step 1301: 0.993511\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.994\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0012695789\n",
      "Minibatch loss at step 1401: 0.997000\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.997\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0013235807\n",
      "Minibatch loss at step 1501: 1.001702\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.0025260448\n",
      "Minibatch loss at step 1601: 0.998215\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.998\n",
      "Test accuracy: 1.0012246370\n",
      "Minibatch loss at step 1701: 0.940640\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.941\n",
      "Validation accuracy: 0.895\n",
      "Test accuracy: 0.8983456492\n",
      "Minibatch loss at step 1801: 0.570656\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.571\n",
      "Validation accuracy: 0.593\n",
      "Test accuracy: 0.5974853635\n",
      "Minibatch loss at step 1901: 0.486984\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.487\n",
      "Validation accuracy: 0.313\n",
      "Test accuracy: 0.3468847275\n",
      "Minibatch loss at step 2001: 0.308239\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.308\n",
      "Validation accuracy: 0.318\n",
      "Test accuracy: 0.3260008991\n",
      "Minibatch loss at step 2101: 0.300257\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.300\n",
      "Validation accuracy: 0.379\n",
      "Test accuracy: 0.3816456497\n",
      "Minibatch loss at step 2201: 0.276921\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.277\n",
      "Validation accuracy: 0.295\n",
      "Test accuracy: 0.2958601117\n",
      "Minibatch loss at step 2301: 0.221482\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.221\n",
      "Validation accuracy: 0.250\n",
      "Test accuracy: 0.2499954849\n",
      "Minibatch loss at step 2401: 0.256346\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.256\n",
      "Validation accuracy: 0.213\n",
      "Test accuracy: 0.2143336236\n",
      "Minibatch loss at step 2501: 0.230278\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.230\n",
      "Validation accuracy: 0.181\n",
      "Test accuracy: 0.1707540601\n",
      "Minibatch loss at step 2601: 0.199353\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.199\n",
      "Validation accuracy: 0.185\n",
      "Test accuracy: 0.1780930459\n",
      "Minibatch loss at step 2701: 0.214700\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.215\n",
      "Validation accuracy: 0.162\n",
      "Test accuracy: 0.1585991234\n",
      "Minibatch loss at step 2801: 0.152883\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.153\n",
      "Validation accuracy: 0.192\n",
      "Test accuracy: 0.1875673383\n",
      "Minibatch loss at step 2901: 0.171765\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.172\n",
      "Validation accuracy: 0.198\n",
      "Test accuracy: 0.1402837187\n",
      "Minibatch loss at step 3001: 0.108579\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.109\n",
      "Validation accuracy: 0.157\n",
      "Test accuracy: 0.1504531354\n",
      "Minibatch loss at step 3101: 0.123295\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.123\n",
      "Validation accuracy: 0.168\n",
      "Test accuracy: 0.1424536407\n",
      "Minibatch loss at step 3201: 0.115705\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.116\n",
      "Validation accuracy: 0.147\n",
      "Test accuracy: 0.1284249127\n",
      "Minibatch loss at step 3301: 0.125061\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.125\n",
      "Validation accuracy: 0.133\n",
      "Test accuracy: 0.1207657829\n",
      "Minibatch loss at step 3401: 0.116969\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.117\n",
      "Validation accuracy: 0.124\n",
      "Test accuracy: 0.1045267135\n",
      "Minibatch loss at step 3501: 0.115199\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.115\n",
      "Validation accuracy: 0.127\n",
      "Test accuracy: 0.1086674631\n",
      "Minibatch loss at step 3601: 0.060458\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.060\n",
      "Validation accuracy: 0.144\n",
      "Test accuracy: 0.1332257688\n",
      "Minibatch loss at step 3701: 0.095030\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.095\n",
      "Validation accuracy: 0.125\n",
      "Test accuracy: 0.1080947071\n",
      "Minibatch loss at step 3801: 0.074898\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.075\n",
      "Validation accuracy: 0.133\n",
      "Test accuracy: 0.1148448735\n",
      "Minibatch loss at step 3901: 0.072682\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.073\n",
      "Validation accuracy: 0.133\n",
      "Test accuracy: 0.1155611947\n",
      "Minibatch loss at step 4001: 0.064482\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.064\n",
      "Validation accuracy: 0.120\n",
      "Test accuracy: 0.1082282811\n",
      "Minibatch loss at step 4101: 0.076123\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.076\n",
      "Validation accuracy: 0.115\n",
      "Test accuracy: 0.1032461524\n",
      "Minibatch loss at step 4201: 0.076451\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.076\n",
      "Validation accuracy: 0.107\n",
      "Test accuracy: 0.0969596654\n",
      "Minibatch loss at step 4301: 0.078915\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.079\n",
      "Validation accuracy: 0.110\n",
      "Test accuracy: 0.1060350165\n",
      "Minibatch loss at step 4401: 0.087692\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.088\n",
      "Validation accuracy: 0.100\n",
      "Test accuracy: 0.0894032493\n",
      "Minibatch loss at step 4501: 0.090705\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.091\n",
      "Validation accuracy: 0.096\n",
      "Test accuracy: 0.0842498615\n",
      "Minibatch loss at step 4601: 0.060437\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.060\n",
      "Validation accuracy: 0.102\n",
      "Test accuracy: 0.0902042463\n",
      "Minibatch loss at step 4701: 0.080914\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.081\n",
      "Validation accuracy: 0.096\n",
      "Test accuracy: 0.0831034258\n",
      "Minibatch loss at step 4801: 0.056087\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.056\n",
      "Validation accuracy: 0.097\n",
      "Test accuracy: 0.0885743424\n",
      "Minibatch loss at step 4901: 0.041656\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.042\n",
      "Validation accuracy: 0.100\n",
      "Test accuracy: 0.0852197036\n",
      "Minibatch loss at step 5001: 0.065597\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.066\n",
      "Validation accuracy: 0.100\n",
      "Test accuracy: 0.0889515430\n",
      "Minibatch loss at step 5101: 0.036598\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.037\n",
      "Validation accuracy: 0.106\n",
      "Test accuracy: 0.0869528875\n",
      "Minibatch loss at step 5201: 0.056594\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.057\n",
      "Validation accuracy: 0.104\n",
      "Test accuracy: 0.0880319327\n",
      "Minibatch loss at step 5301: 0.035643\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.036\n",
      "Validation accuracy: 0.099\n",
      "Test accuracy: 0.0872026533\n",
      "Minibatch loss at step 5401: 0.036152\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.036\n",
      "Validation accuracy: 0.100\n",
      "Test accuracy: 0.0845694542\n",
      "Minibatch loss at step 5501: 0.013345\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.013\n",
      "Validation accuracy: 0.108\n",
      "Test accuracy: 0.0879718214\n",
      "Minibatch loss at step 5601: 0.014442\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.014\n",
      "Validation accuracy: 0.100\n",
      "Test accuracy: 0.0834364146\n",
      "Minibatch loss at step 5701: 0.033867\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.034\n",
      "Validation accuracy: 0.098\n",
      "Test accuracy: 0.0829273313\n",
      "Minibatch loss at step 5801: 0.056184\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.056\n",
      "Validation accuracy: 0.099\n",
      "Test accuracy: 0.0829958990\n",
      "Minibatch loss at step 5901: 0.046200\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.046\n",
      "Validation accuracy: 0.099\n",
      "Test accuracy: 0.0840923041\n",
      "Minibatch loss at step 6001: 0.076692\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.077\n",
      "Validation accuracy: 0.100\n",
      "Test accuracy: 0.0841970220\n",
      "Minibatch loss at step 6101: 0.032673\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.033\n",
      "Validation accuracy: 0.100\n",
      "Test accuracy: 0.0846224278\n",
      "Minibatch loss at step 6201: 0.069890\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.101\n",
      "Test accuracy: 0.0848220140\n",
      "Minibatch loss at step 6301: 0.043248\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.043\n",
      "Validation accuracy: 0.101\n",
      "Test accuracy: 0.0850032270\n",
      "Minibatch loss at step 6401: 0.049360\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.049\n",
      "Validation accuracy: 0.103\n",
      "Test accuracy: 0.0852318928\n",
      "Minibatch loss at step 6501: 0.058630\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.059\n",
      "Validation accuracy: 0.103\n",
      "Test accuracy: 0.0857530534\n",
      "Minibatch loss at step 6601: 0.065399\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.065\n",
      "Validation accuracy: 0.103\n",
      "Test accuracy: 0.0857965350\n",
      "Minibatch loss at step 6701: 0.064426\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.064\n",
      "Validation accuracy: 0.103\n",
      "Test accuracy: 0.0860327780\n",
      "Minibatch loss at step 6801: 0.031679\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.032\n",
      "Validation accuracy: 0.103\n",
      "Test accuracy: 0.0860937759\n",
      "Minibatch loss at step 6901: 0.041941\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.042\n",
      "Validation accuracy: 0.105\n",
      "Test accuracy: 0.0864664465\n",
      "Minibatch loss at step 7001: 0.010542\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.011\n",
      "Validation accuracy: 0.102\n",
      "Test accuracy: 0.0862546787\n",
      "Minibatch loss at step 7101: 0.029057\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.029\n",
      "Validation accuracy: 0.105\n",
      "Test accuracy: 0.0868771523\n",
      "Minibatch loss at step 7201: 0.010739\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.011\n",
      "Validation accuracy: 0.102\n",
      "Test accuracy: 0.0845209435\n",
      "Minibatch loss at step 7301: 0.057621\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.058\n",
      "Validation accuracy: 0.103\n",
      "Test accuracy: 0.0853642970\n",
      "Minibatch loss at step 7401: 0.041405\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.041\n",
      "Validation accuracy: 0.104\n",
      "Test accuracy: 0.0857087076\n",
      "Minibatch loss at step 7501: 0.067534\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.068\n",
      "Validation accuracy: 0.105\n",
      "Test accuracy: 0.0860292092\n",
      "Minibatch loss at step 7601: 0.041103\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.041\n",
      "Validation accuracy: 0.105\n",
      "Test accuracy: 0.0862522721\n",
      "Minibatch loss at step 7701: 0.070471\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.070\n",
      "Validation accuracy: 0.106\n",
      "Test accuracy: 0.0864428505\n",
      "Minibatch loss at step 7801: 0.039660\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.040\n",
      "Validation accuracy: 0.106\n",
      "Test accuracy: 0.0865713954\n",
      "Minibatch loss at step 7901: 0.039733\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.040\n",
      "Validation accuracy: 0.107\n",
      "Test accuracy: 0.0867000148\n",
      "Minibatch loss at step 8001: 0.047117\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.047\n",
      "Validation accuracy: 0.108\n",
      "Test accuracy: 0.0870426595\n",
      "Minibatch loss at step 8101: 0.064705\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.065\n",
      "Validation accuracy: 0.107\n",
      "Test accuracy: 0.0870621279\n",
      "Minibatch loss at step 8201: 0.070869\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.071\n",
      "Validation accuracy: 0.108\n",
      "Test accuracy: 0.0872145966\n",
      "Minibatch loss at step 8301: 0.039477\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.039\n",
      "Validation accuracy: 0.108\n",
      "Test accuracy: 0.0872577801\n",
      "Minibatch loss at step 8401: 0.039180\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.039\n",
      "Validation accuracy: 0.109\n",
      "Test accuracy: 0.0875293761\n",
      "Minibatch loss at step 8501: 0.027231\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.027\n",
      "Validation accuracy: 0.108\n",
      "Test accuracy: 0.0874026492\n",
      "Minibatch loss at step 8601: 0.027540\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.028\n",
      "Validation accuracy: 0.109\n",
      "Test accuracy: 0.0877760649\n",
      "Minibatch loss at step 8701: 0.010142\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.010\n",
      "Validation accuracy: 0.109\n",
      "Test accuracy: 0.0875974894\n",
      "Minibatch loss at step 8801: 0.060363\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.060\n",
      "Validation accuracy: 0.110\n",
      "Test accuracy: 0.0878796801\n",
      "Minibatch loss at step 8901: 0.038759\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.039\n",
      "Validation accuracy: 0.110\n",
      "Test accuracy: 0.0878994912\n",
      "Minibatch loss at step 9001: 0.060752\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.061\n",
      "Validation accuracy: 0.111\n",
      "Test accuracy: 0.0880900621\n",
      "Minibatch loss at step 9101: 0.030024\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.030\n",
      "Validation accuracy: 0.110\n",
      "Test accuracy: 0.0880829766\n",
      "Minibatch loss at step 9201: 0.075156\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.075\n",
      "Validation accuracy: 0.111\n",
      "Test accuracy: 0.0882189795\n",
      "Minibatch loss at step 9301: 0.038169\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.038\n",
      "Validation accuracy: 0.110\n",
      "Test accuracy: 0.0882483721\n",
      "Minibatch loss at step 9401: 0.037975\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.038\n",
      "Validation accuracy: 0.111\n",
      "Test accuracy: 0.0883975700\n",
      "Minibatch loss at step 9501: 0.045353\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.045\n",
      "Validation accuracy: 0.111\n",
      "Test accuracy: 0.0884909183\n",
      "Minibatch loss at step 9601: 0.058531\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.059\n",
      "Validation accuracy: 0.111\n",
      "Test accuracy: 0.0884943008\n",
      "Minibatch loss at step 9701: 0.063330\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.063\n",
      "Validation accuracy: 0.112\n",
      "Test accuracy: 0.0886044279\n",
      "Minibatch loss at step 9801: 0.045140\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.045\n",
      "Validation accuracy: 0.112\n",
      "Test accuracy: 0.0886582211\n",
      "Minibatch loss at step 9901: 0.057344\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.057\n",
      "Validation accuracy: 0.112\n",
      "Test accuracy: 0.0887304693\n",
      "Minibatch loss at step 10001: 0.037598\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.038\n",
      "Validation accuracy: 0.112\n",
      "Test accuracy: 0.0887365714\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return np.sqrt(((predictions - labels) ** 2).mean())\n",
    "\n",
    "def accuracy_xy(predictions,labels):\n",
    "    return (np.sqrt(((predictions[:,0] - labels[:.0]) ** 2).mean()),np.sqrt(((predictions[:,1] - labels[:,1]) ** 2).mean()))\n",
    "    \n",
    "num_steps = 10002\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions,l1 = session.run(\n",
    "      [optimizer, loss, train_prediction,w], feed_dict=feed_dict)\n",
    "    if (step % 100 == 1):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(l1[1].shape)\n",
    "        \n",
    "      print(\"Minibatch accuracy: %.3f\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.3f\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.10f\" % accuracy_xy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

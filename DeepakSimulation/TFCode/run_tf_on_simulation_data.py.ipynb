{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import scipy.io as spio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Import Data from Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "744\n"
     ]
    }
   ],
   "source": [
    "mat = spio.loadmat('../ReflectionModel/dataset_2.mat')\n",
    "features = mat['features']\n",
    "labels = mat['labels']\n",
    "print(len(features))\n",
    "print(len(features[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess Data to make it 0-mean, 1 variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(744,)\n",
      "(100000, 744)\n",
      "(100000, 744)\n",
      "(100000, 2)\n",
      "[ 5.75757782  4.33572888]\n",
      "[ 1.50315946  1.50109093  1.4988298   1.49266982  1.48432403  1.47321853\n",
      "  1.45962753  1.44957145  1.43547674  1.42550123  1.41776869  1.40891897\n",
      "  1.40464574  1.40081011  1.39350686  1.3876605   1.38834796  1.38462617\n",
      "  1.38282034  1.37929115  1.37467546  1.36807602  1.36257241  1.36075581\n",
      "  1.35685574  1.35138349  1.34216772  1.33444303  1.3243377   1.31200297\n",
      "  1.29890701  1.48734615  1.48479231  1.48126276  1.47927189  1.47873322\n",
      "  1.47363626  1.47069967  1.47064985  1.47103399  1.46945672  1.46635492\n",
      "  1.46368271  1.45621113  1.44749263  1.44078627  1.43670867  1.43340685\n",
      "  1.4299623   1.4269948   1.42076747  1.41534622  1.40888781  1.40179645\n",
      "  1.39467555  1.38957635  1.38892293  1.38715744  1.38159447  1.37412147\n",
      "  1.36516209  1.35865435  1.86039562  1.84183167  1.8274101   1.81269607\n",
      "  1.79934722  1.78957177  1.78205275  1.77188538  1.7611249   1.74892173\n",
      "  1.73490111  1.72492656  1.7168491   1.70649949  1.69336992  1.68001034\n",
      "  1.66951048  1.65945707  1.64609551  1.63373737  1.62212856  1.61164676\n",
      "  1.6002171   1.59313509  1.58363206  1.57522532  1.56687299  1.55996765\n",
      "  1.5518334   1.5433333   1.53394917  1.67620341  1.6706      1.66722331\n",
      "  1.66691381  1.66413063  1.65846909  1.65250379  1.64611847  1.64040373\n",
      "  1.63639823  1.63460168  1.62921741  1.62706554  1.62103379  1.61421673\n",
      "  1.60812408  1.60094143  1.5943891   1.58644085  1.5761027   1.56613345\n",
      "  1.55512864  1.54913204  1.53989759  1.5330214   1.52766477  1.51777628\n",
      "  1.51015413  1.50441535  1.50069783  1.49639636  1.64049225  1.63360127\n",
      "  1.6272581   1.6204577   1.61349309  1.61012833  1.60589933  1.59912088\n",
      "  1.59255954  1.58448362  1.57997066  1.5755321   1.57121733  1.56710699\n",
      "  1.56243945  1.55774213  1.55386512  1.54714875  1.54347302  1.54152111\n",
      "  1.54049593  1.53589026  1.53196872  1.52877287  1.52575643  1.52357798\n",
      "  1.51876377  1.5152143   1.51425103  1.51169642  1.50610375  1.64159451\n",
      "  1.63683952  1.63151935  1.6229891   1.61620825  1.60462789  1.5938856\n",
      "  1.58282721  1.57210465  1.56152943  1.55258617  1.54417418  1.53395898\n",
      "  1.5233088   1.51264498  1.50703244  1.50656133  1.5043361   1.50197531\n",
      "  1.49731014  1.49470717  1.49174775  1.48581706  1.47802547  1.4732194\n",
      "  1.46515427  1.45933922  1.45509521  1.44729801  1.44014233  1.43286831\n",
      "  1.65565356  1.6519178   1.65299135  1.64967213  1.65058984  1.64898455\n",
      "  1.6458768   1.64277633  1.63659521  1.63176281  1.62711595  1.62377311\n",
      "  1.61619712  1.61049036  1.60358799  1.59667288  1.59107295  1.59195848\n",
      "  1.59136702  1.59134341  1.59253236  1.59271428  1.59168339  1.59100269\n",
      "  1.5888244   1.58331165  1.58111531  1.57811541  1.57059827  1.56264609\n",
      "  1.55611102  1.96174272  1.94855758  1.93719007  1.92635737  1.91864507\n",
      "  1.90970988  1.89719118  1.88478077  1.87569444  1.86619989  1.85828538\n",
      "  1.84920823  1.837978    1.82694656  1.81613684  1.80784708  1.79894598\n",
      "  1.78715818  1.77607177  1.76408672  1.75167233  1.74291146  1.73883963\n",
      "  1.73282702  1.72569489  1.72027972  1.71459358  1.70909452  1.70105224\n",
      "  1.69234652  1.68375257  1.66032219  1.65143736  1.64381643  1.63554498\n",
      "  1.62788331  1.61955307  1.61402173  1.61030915  1.60413822  1.59841851\n",
      "  1.59581012  1.58977324  1.58338685  1.57714342  1.57228558  1.56771805\n",
      "  1.56351613  1.55759921  1.55579888  1.55234227  1.54823816  1.5407871\n",
      "  1.53106008  1.52373454  1.51747659  1.51107275  1.5034967   1.49796581\n",
      "  1.49656024  1.49109799  1.48926531  1.63774458  1.63888256  1.63896807\n",
      "  1.63469714  1.62786353  1.62352538  1.61875403  1.61538615  1.60930817\n",
      "  1.60174845  1.59611346  1.58960424  1.58447084  1.57802759  1.57625318\n",
      "  1.57674261  1.57846632  1.5718483   1.56585731  1.55897064  1.55628703\n",
      "  1.55428864  1.55081539  1.54989736  1.54901438  1.54695833  1.54417359\n",
      "  1.5412055   1.53766884  1.53237687  1.52849102  1.74493009  1.73841124\n",
      "  1.73198494  1.7274207   1.72547629  1.72186811  1.72048451  1.716304\n",
      "  1.70983652  1.70601775  1.70032347  1.69457586  1.68711213  1.67864346\n",
      "  1.66849675  1.66190267  1.65749007  1.65370008  1.65229645  1.64932084\n",
      "  1.6427356   1.63807014  1.63348981  1.62568842  1.61744766  1.60843796\n",
      "  1.60153653  1.5912863   1.58135396  1.57378982  1.56591255  2.03192217\n",
      "  2.01920956  2.0064821   1.99166897  1.97410439  1.95968198  1.94732955\n",
      "  1.93422227  1.92262359  1.91353376  1.90394984  1.89270355  1.8818348\n",
      "  1.87173558  1.86175211  1.85177458  1.8403812   1.83446383  1.82678974\n",
      "  1.81919013  1.81175702  1.80491651  1.7954975   1.78434677  1.77315322\n",
      "  1.76281021  1.75191549  1.7432965   1.73388092  1.7247004   1.71573872\n",
      "  2.10525425  2.09340579  2.08191185  2.07329069  2.06657503  2.06161416\n",
      "  2.05865681  2.05330916  2.05069052  2.04531762  2.03871667  2.03280234\n",
      "  2.02382376  2.01454802  2.00798513  2.00028753  1.98830539  1.97933231\n",
      "  1.96922534  1.96030032  1.95220854  1.94580058  1.93846455  1.92897714\n",
      "  1.92065454  1.91345792  1.9093114   1.90412973  1.90051989  1.89872067\n",
      "  1.89721827  1.42557464  1.41592317  1.40717303  1.39707438  1.38560171\n",
      "  1.37884252  1.37011024  1.35832189  1.34561035  1.33565438  1.32724361\n",
      "  1.3183053   1.31498539  1.31309103  1.30908189  1.30230803  1.29457688\n",
      "  1.28715815  1.27921864  1.27531436  1.27005915  1.26644948  1.26379731\n",
      "  1.26084673  1.25594636  1.24603616  1.23744746  1.23311272  1.23110475\n",
      "  1.2310165   1.22803384  2.27654373  2.27571186  2.27209361  2.26818285\n",
      "  2.26364271  2.25645884  2.24742016  2.24069592  2.23444952  2.22946432\n",
      "  2.22626548  2.21964038  2.21174271  2.20573955  2.20231866  2.19847361\n",
      "  2.19310194  2.18706329  2.18409105  2.17991886  2.1757182   2.17045856\n",
      "  2.16606115  2.15872145  2.1530271   2.14662081  2.14034731  2.1330853\n",
      "  2.12663537  2.12084314  2.11576031  1.75546817  1.74700564  1.73627148\n",
      "  1.72288559  1.71194312  1.70373624  1.69609023  1.68899913  1.68133458\n",
      "  1.6718638   1.66054458  1.65289254  1.64213974  1.63503274  1.62928929\n",
      "  1.62254353  1.61726014  1.61125055  1.60658868  1.60453198  1.60215377\n",
      "  1.60114327  1.59497331  1.59206152  1.58712168  1.58083684  1.57884083\n",
      "  1.57479398  1.5688141   1.56127637  1.55434886  1.59886904  1.59259276\n",
      "  1.58554127  1.57902777  1.57293019  1.56324383  1.55451798  1.54830212\n",
      "  1.54230497  1.53793597  1.52986679  1.52162566  1.51386718  1.50570844\n",
      "  1.4981431   1.49068644  1.48260868  1.47775357  1.46932106  1.4594871\n",
      "  1.44864673  1.44190745  1.43452474  1.42597715  1.4175295   1.40819984\n",
      "  1.40184197  1.39452806  1.38426088  1.37562905  1.37041344  1.49595937\n",
      "  1.48742068  1.47966783  1.47561043  1.46975411  1.46923058  1.46805998\n",
      "  1.46688575  1.46577909  1.46434465  1.46139376  1.45766515  1.45635411\n",
      "  1.45540515  1.45462164  1.44845208  1.43730884  1.42778252  1.41879886\n",
      "  1.41209629  1.40339371  1.39497826  1.38982401  1.38673752  1.38084933\n",
      "  1.37840801  1.37336441  1.36685782  1.36437055  1.36111482  1.35821127\n",
      "  1.86971209  1.85920867  1.84452527  1.83399471  1.81934034  1.80707535\n",
      "  1.79652654  1.78625709  1.77856616  1.76979428  1.76079013  1.75090704\n",
      "  1.7450127   1.73743489  1.73119877  1.72506024  1.71763719  1.70453501\n",
      "  1.69238759  1.68027873  1.6668478   1.6546381   1.64339742  1.63196118\n",
      "  1.62180875  1.61533454  1.60567706  1.5966722   1.59216411  1.58831634\n",
      "  1.58316716  2.27069129  2.2657084   2.25899762  2.25225001  2.2428911\n",
      "  2.23466377  2.22944807  2.22457313  2.21679241  2.20962974  2.20113619\n",
      "  2.19363054  2.18829051  2.18272774  2.1772627   2.16971887  2.1627908\n",
      "  2.15849234  2.15340984  2.14939693  2.14570958  2.13925894  2.12893384\n",
      "  2.12025586  2.11261267  2.10380707  2.09533603  2.08668158  2.08027272\n",
      "  2.07441451  2.06884918  1.59620307  1.5916127   1.585646    1.58104818\n",
      "  1.57553729  1.57076758  1.56326385  1.55396187  1.54717744  1.54044115\n",
      "  1.53055487  1.52395126  1.51773704  1.51180167  1.50458054  1.49713054\n",
      "  1.48908893  1.48310463  1.47311063  1.46454757  1.45688908  1.45286925\n",
      "  1.45152697  1.44755269  1.44271339  1.4380637   1.43457332  1.42889418\n",
      "  1.41946225  1.4139467   1.40485676  1.85855425  1.84381168  1.83020562\n",
      "  1.82053756  1.81289007  1.80352175  1.79473162  1.78430947  1.77678256\n",
      "  1.7706692   1.76303644  1.75594599  1.74796175  1.74129753  1.73043877\n",
      "  1.71738836  1.70338139  1.69729254  1.69049756  1.68491237  1.67529983\n",
      "  1.66530315  1.65644728  1.64560035  1.63480081  1.62485637  1.61607666\n",
      "  1.60712585  1.59909605  1.59270889  1.58514543  1.69025569  1.68253532\n",
      "  1.67498005  1.66517296  1.65299395  1.6427391   1.630149    1.62053339\n",
      "  1.61348189  1.60398015  1.59623251  1.5887426   1.58327074  1.57898031\n",
      "  1.57652436  1.57051825  1.56201179  1.55316077  1.5416191   1.53201834\n",
      "  1.52649482  1.51886891  1.51135586  1.50735807  1.50384767  1.50117417\n",
      "  1.49656846  1.49565911  1.494113    1.49049973  1.48703683  2.32391722\n",
      "  2.31810141  2.31246428  2.30873081  2.30707454  2.30309422  2.29751976\n",
      "  2.29267445  2.28679473  2.27869936  2.27112184  2.26516916  2.2590215\n",
      "  2.25221653  2.24536595  2.23915905  2.2336747   2.22408182  2.21582876\n",
      "  2.20777158  2.19949241  2.19110954  2.184757    2.17986481  2.17538416\n",
      "  2.17013135  2.16539471  2.15897847  2.15321154  2.14742484  2.14181888]\n"
     ]
    }
   ],
   "source": [
    "feature_mean = np.mean(features,axis=0)\n",
    "label_mean = np.mean(labels,axis=0)\n",
    "label_std = np.std(labels,axis=0)\n",
    "feature_std = np.std(features,axis=0)\n",
    "print(feature_std.shape)\n",
    "feature_mean_m = np.tile(feature_mean, [len(features),1])\n",
    "feature_std_m = np.tile(feature_std, [len(features),1])\n",
    "label_mean_m = np.tile(label_mean,[len(labels),1])\n",
    "label_std_m = np.tile(label_std,[len(labels),1])\n",
    "print(feature_std_m.shape)\n",
    "features_normal = np.divide(features - feature_mean_m, feature_std_m)\n",
    "labels_normal = np.divide(labels- label_mean_m, label_std_m)\n",
    "print(features_normal.shape)\n",
    "print(labels_normal.shape)\n",
    "print(label_std)\n",
    "print(feature_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.float64'>\n",
      "<type 'numpy.float32'>\n",
      "(60000, 744)\n",
      "[[-0.58843237  1.04194999  1.55480695  0.34897208 -1.17734814 -1.47347283\n",
      "  -0.15485772  1.35086918  1.43462837 -0.06087759]\n",
      " [ 0.5727374  -0.63181102  0.57865942 -0.48796439  0.29917082 -0.0819752\n",
      "  -0.12569059  0.34876576 -0.49538198  0.58206981]\n",
      " [ 0.38792133 -0.09478956 -0.33191782  0.58606082 -0.70387477  0.53310448\n",
      "  -0.23637578 -0.18864956  0.47202417 -0.68435276]\n",
      " [ 0.4830538  -0.41351202  0.27046362 -0.18620938  0.04199331  0.12849844\n",
      "  -0.26418701  0.37733597 -0.46469939  0.56523752]\n",
      " [ 0.23897913  0.29706541 -0.21316612 -0.31462404  0.16421017  0.36679238\n",
      "  -0.11064763 -0.33104613  0.09344901  0.36327749]\n",
      " [ 0.47181526 -0.51018226  0.46705303 -0.4285498   0.29008842 -0.14044641\n",
      "   0.02227816  0.12073169 -0.25261331  0.38072529]\n",
      " [-0.39212355  0.21411426  0.09052867 -0.35841307  0.44956285 -0.35605821\n",
      "   0.10936376  0.16798821 -0.39403951  0.50060964]\n",
      " [ 0.06375302  0.13484873 -0.30906481  0.44539991 -0.59131849  0.60660297\n",
      "  -0.60562086  0.54088175 -0.43001989  0.24821724]\n",
      " [-1.00214684 -0.46104035  0.99861318  0.41289398 -1.01914525 -0.3642562\n",
      "   1.03837085  0.34895554 -1.05346155 -0.34890807]\n",
      " [ 0.34256652 -0.07658372 -0.25627568  0.5011549  -0.6298157   0.62589526\n",
      "  -0.45365131  0.16558865  0.18203819 -0.47946042]]\n"
     ]
    }
   ],
   "source": [
    "train_pts = 60000\n",
    "valid_pts = 20000\n",
    "test_pts = 20000\n",
    "print(type(features_normal[0][0]))\n",
    "features_normal = features_normal.astype('float32')\n",
    "print(type(features_normal[0][0]))\n",
    "labels_normal = labels_normal.astype('float32')\n",
    "train_dataset = (features_normal[0:train_pts ,:])\n",
    "train_labels = labels_normal[0:train_pts,:]\n",
    "valid_dataset = features_normal[train_pts:valid_pts+train_pts,:]\n",
    "valid_labels = labels_normal[train_pts:valid_pts+train_pts,:]\n",
    "test_dataset = features_normal[train_pts+valid_pts:train_pts+valid_pts+test_pts,:]\n",
    "test_labels = labels_normal[train_pts+valid_pts:train_pts+valid_pts+test_pts,:]\n",
    "print(train_dataset.shape)\n",
    "print(train_dataset[0:10,0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup TensorFlow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "744\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2048\n",
    "num_hidden = [1024,512,256]\n",
    "depth = 4\n",
    "graph = tf.Graph()\n",
    "keep_prob = 1\n",
    "decay_rate = 0.5\n",
    "decay_steps = 1500\n",
    "num_features =features.shape[1]\n",
    "num_labels = labels.shape[1]\n",
    "print(num_features)\n",
    "def get_logits_for_training(dataset,w,b):\n",
    "    cur_mat = dataset\n",
    "    for i in range(0,depth-1):\n",
    "        cur_mat = tf.matmul(cur_mat,w[i])+b[i]\n",
    "        cur_mat = tf.nn.dropout(tf.nn.relu(cur_mat),keep_prob)\n",
    "    logits = tf.matmul(cur_mat,w[depth-1])+b[depth-1]\n",
    "    return logits\n",
    "\n",
    "def get_logits_for_prediction(dataset,w,b):\n",
    "    cur_mat = dataset\n",
    "    for i in range(0,depth-1):\n",
    "        cur_mat = tf.matmul(cur_mat,w[i])+b[i]\n",
    "        cur_mat = tf.nn.relu(cur_mat)\n",
    "    logits = tf.matmul(cur_mat,w[depth-1])+b[depth-1]\n",
    "    return logits\n",
    "    \n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, num_features))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  w = []\n",
    "  b = []\n",
    "  w.append(tf.Variable(tf.truncated_normal([num_features, num_hidden[0]],stddev=0.002)))\n",
    "  b.append(tf.zeros([num_hidden[0]]))\n",
    "  for i in range(1,depth-1):\n",
    "    w.append(tf.Variable(tf.truncated_normal([num_hidden[i-1],num_hidden[i]], stddev=0.002)))\n",
    "    b.append(tf.zeros([num_hidden[i]]))\n",
    "  w.append(tf.Variable(tf.truncated_normal([num_hidden[depth-2], num_labels],stddev=0.002)))\n",
    "  b.append(tf.Variable(tf.zeros([num_labels])))\n",
    "  \n",
    "  # Training computation.  \n",
    "  logits = get_logits_for_training(tf_train_dataset,w,b)\n",
    "  #loss = tf.reduce_mean(\n",
    "  #  tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "  loss = tf.sqrt(tf.reduce_mean(tf.squared_difference(tf_train_labels, logits)))\n",
    "  #Optimizer\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, decay_steps, decay_rate)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  logits_predict = get_logits_for_prediction(tf_train_dataset,w,b)\n",
    "  train_prediction = (logits_predict)\n",
    "  valid_prediction = (get_logits_for_prediction(tf_valid_dataset,w,b))\n",
    "  test_prediction = (get_logits_for_prediction(tf_test_dataset,w,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 1: 1.002859\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 101: 0.993718\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.994\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 201: 0.997682\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 301: 0.995493\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.995\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 401: 0.997666\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 501: 0.994685\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.995\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 601: 1.002511\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 701: 0.998541\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 801: 1.005485\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.005\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 901: 1.002511\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1001: 1.005898\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.006\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1101: 0.994687\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.995\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1201: 1.009314\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.009\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1301: 0.993514\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.994\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1401: 0.997001\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.997\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1501: 1.000894\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.001\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1601: 0.998471\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1701: 0.991534\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.992\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1801: 0.990581\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.991\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 1901: 0.999304\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2001: 0.992864\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.993\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2101: 1.006860\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.007\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2201: 0.997969\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2301: 0.998884\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2401: 1.001506\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2501: 1.003472\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2601: 0.996726\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.997\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2701: 1.007600\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.008\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2801: 0.995610\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.996\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 2901: 0.998519\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3001: 1.001651\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3101: 1.002296\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3201: 0.991677\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.992\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3301: 0.990797\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.991\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3401: 0.993074\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.993\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3501: 0.998515\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3601: 0.998043\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3701: 1.003094\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3801: 0.994220\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.994\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 3901: 1.010619\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.011\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4001: 1.002061\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4101: 0.998530\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4201: 1.006684\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.007\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4301: 0.994698\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.995\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4401: 1.004833\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.005\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4501: 0.998730\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4601: 1.002415\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4701: 0.990976\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.991\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4801: 0.994856\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.995\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 4901: 0.997009\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.997\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5001: 0.995909\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.996\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5101: 0.999567\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.000\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5201: 1.008232\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.008\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5301: 0.990595\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.991\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5401: 1.014588\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.015\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5501: 1.002446\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5601: 0.996831\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.997\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5701: 1.006845\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.007\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5801: 1.005202\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.005\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 5901: 0.999023\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6001: 0.992789\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.993\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6101: 1.009928\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.010\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6201: 0.996968\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.997\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6301: 1.003233\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6401: 0.999247\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6501: 0.994167\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.994\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6601: 1.002280\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6701: 1.000623\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.001\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6801: 0.981924\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.982\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 6901: 1.008277\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.008\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7001: 1.006326\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.006\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7101: 0.997971\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7201: 1.004239\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.004\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7301: 1.000283\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.000\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7401: 1.001536\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7501: 0.993609\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.994\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7601: 1.008983\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.009\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7701: 1.004605\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.005\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7801: 1.002748\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 7901: 0.996559\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.997\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8001: 0.988329\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.988\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8101: 1.009352\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.009\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8201: 1.001065\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.001\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8301: 0.991605\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.992\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8401: 1.004128\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.004\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8501: 1.005680\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.006\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8601: 1.002053\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8701: 1.001692\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.002\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8801: 1.001302\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.001\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 8901: 1.003174\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.003\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9001: 0.995880\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.996\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9101: 1.000273\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.000\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9201: 1.004801\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.005\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9301: 0.999157\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9401: 0.998182\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.998\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9501: 0.988849\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.989\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9601: 1.012383\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.012\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9701: 0.999090\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9801: 0.993111\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.993\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 9901: 0.998884\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 0.999\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n",
      "Minibatch loss at step 10001: 1.011280\n",
      "(1024, 512)\n",
      "Minibatch accuracy: 1.011\n",
      "Validation accuracy: 0.999\n",
      "Test accuracy: 1.001\n"
     ]
    }
   ],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return np.sqrt(((predictions - labels) ** 2).mean())\n",
    "num_steps = 10002\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions,l1 = session.run(\n",
    "      [optimizer, loss, train_prediction,w], feed_dict=feed_dict)\n",
    "    if (step % 100 == 1):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(l1[1].shape)\n",
    "        \n",
    "      print(\"Minibatch accuracy: %.3f\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.3f\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.3f\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
